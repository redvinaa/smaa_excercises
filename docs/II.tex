\section{Kernel methods}

%% Linear classification {{{
\subsection{Linear classification}

At first, we create three datasets, with different distances between
the means of two classes, so that there is one with no overlap,
one with heavy overlap and one semi-overlapping.
Each dataset consists of the coordinates of the points $x_i\in\mathbb{R}^2$,
and the corresponding classification $y_i\in\{-1, 1\}$.

Three classifiers are implemented.

\subsubsection*{Soft Margin Support Vector Classification}

% With the original SVC, we are looking for the parameters of the affine function
% that best separates the two classes: $\sum\limits_{i=1}^ny_i\brc{w\T\,x_ib}=0$.
% This way, the separating line is going to be halfway between two specific
% points of the two classes. The distance between the line and these vectors
% has to be maximized, and that is done by minimizing $w$.
% However, this can only be done on linearly separable datasets,
% so a different solution is needed.

The support vector classifier is a modification of Vapnik's SVC, such that
badly classified data points are penalized.

The problem can be solved via convex optimization, using the cvxpy python library.
\begin{equation}
\begin{split}
	&\underset{w,\,\epsilon\in\mathbb{R}^d,\,b\in\mathbb{R}}{\operatorname{minimize}}\HS
		\frac{1}{2}\,\norm{w}^2 + \lambda\,\sum\epsilon\\
	&\text{subject to}\HS
		y_k\,\brc{w\T\,x_k + b}\geq1-\epsilon_k\hs\text{and}\hs\epsilon_k\geq0\HS\text{for}\hs k=1,\dots n
\end{split}
\end{equation}

\subsubsection*{Least Squares SVM}

LS-SVM is the least squares reformulation of Vapnik's SVC.
This too can be solved via convex optimization.
\begin{equation}
\begin{split}
	&\underset{w,\,\epsilon\in\mathbb{R}^d,\,b\in\mathbb{R}}{\operatorname{minimize}}\HS
		\frac{1}{2}\,\norm{w}^2 + \lambda\,\sum\epsilon^2\\
	&\text{subject to}\HS
		y_k\,\brc{w\T\,x_k + b} = 1-\epsilon_k\HS\text{for}\hs k=1,\dots n
\end{split}
\end{equation}

\subsubsection*{Nearest centroid classifier}

With the NNC solution, the means of the classes are calculated.
Now a new sample would be classified in the class whose mean
it is closer to.

The plots of the classifiers applied to the different datasets
can be seen on figure \ref{fig:linear-classification}.

\begin{figure}[H]
	\centering
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim={5cm, 7cm, 5cm, 8cm}, clip]{ex_II_1_plots_0}
		\caption{Heavily overlapping distributions}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim={5cm, 5cm, 5cm, 6cm}, clip]{ex_II_1_plots_1}
		\caption{Semi-overlapping distributions}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim={5cm, 5cm, 5cm, 6cm}, clip]{ex_II_1_plots_2}
		\caption{Non-overlapping distributions}
	\end{subfigure}
	\caption{Linear classification experiments}
	\label{fig:linear-classification}
\end{figure}

%% }}}
